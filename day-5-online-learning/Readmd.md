# ğŸ”„ Online Learning (Incremental Learning)

This repository contains concise and structured notes on **Online Learning**, a machine learning paradigm designed for **streaming, large-scale, and continuously evolving data**. These notes are adapted from my Notion study material and are suitable for **students, interviews, and quick revision**.

---

## ğŸš€ What is Online Learning?

Online Learning is a machine learning approach where the model **updates itself incrementally** as new data arrives. Instead of training once on a full dataset, the model learns **one data point or a small batch at a time**, making it highly adaptive to change.

This approach is ideal when datasets are **too large to fit into memory** or when data is generated continuously.

---

## ğŸ§  Key Characteristics

- Incremental model updates
- Works with **data streams** and mini-batches
- Low memory consumption
- Fast adaptation to new patterns
- Better handling of **concept drift**

---

## â±ï¸ When to Use Online Learning

Online Learning is preferred when:

- ğŸ“Š Data arrives continuously (streaming data)
- ğŸŒ Data distribution changes over time
- ğŸ’¾ Dataset is too large to fit in memory
- âš¡ Real-time or near real-time predictions are required
- ğŸ” Frequent model updates are needed

### âœ… Common Use Cases

- Recommendation systems
- Fraud detection
- Stock price prediction
- Online advertising (CTR prediction)
- IoT and sensor data processing

---

## âš™ï¸ How to Implement Online Learning

### 1ï¸âƒ£ Data Stream Processing

- Process data one instance at a time or in mini-batches
- No assumption of full dataset availability

### 2ï¸âƒ£ Incremental Model Updates

- Update model parameters after each data point
- Commonly uses gradient-based optimization (e.g., SGD)

### 3ï¸âƒ£ Continuous Evaluation

- Uses **prequential evaluation** (predict â†’ learn â†’ evaluate)
- Performance is monitored continuously

### 4ï¸âƒ£ Concept Drift Handling

- Sliding windows
- Decay factors (recent data weighted more)

---

## ğŸ“‰ Learning Rate

### ğŸ”¹ What is Learning Rate?

The **learning rate (Î·)** controls how much model parameters change when new data arrives.

### âš–ï¸ Trade-offs

- ğŸ”¼ High learning rate â†’ Fast learning, unstable updates
- ğŸ”½ Low learning rate â†’ Stable updates, slow adaptation

### ğŸ› ï¸ Common Strategies

- Constant learning rate
- Time-decayed learning rate
- Adaptive optimizers (AdaGrad, RMSProp, Adam)

---

## ğŸ’¾ Out-of-Core Learning

### ğŸ”¹ What is Out-of-Core Learning?

Out-of-Core Learning enables training on datasets that **do not fit into memory** by loading data in **small chunks**.

### ğŸ”— Relation to Online Learning

- Often combined with online or mini-batch learning
- Enables scalable learning on massive datasets

### âœ… Benefits

- Minimal memory usage
- Suitable for big data environments

---

## âš ï¸ Disadvantages of Online Learning

- âŒ Sensitive to noisy and outlier data
- âŒ Difficult to debug due to continuous updates
- âŒ Highly sensitive to learning rate selection
- âŒ Risk of catastrophic forgetting
- âŒ Complex evaluation compared to batch learning

---

## ğŸ” Online Learning vs Batch Learning

| Feature       | Online Learning | Batch Learning |
| ------------- | --------------- | -------------- |
| Learning Type | Incremental     | Offline        |
| Data Handling | Streaming       | Static         |
| Adaptability  | High            | Low            |
| Memory Usage  | Low             | High           |
| Concept Drift | Better Handling | Poor Handling  |

---

## ğŸ“ Summary

Online Learning is powerful for **dynamic and real-time systems**, offering adaptability and scalability. However, it requires careful tuning, monitoring, and evaluation to avoid instability.

---

â­ If you find these notes useful, consider starring the repository!
